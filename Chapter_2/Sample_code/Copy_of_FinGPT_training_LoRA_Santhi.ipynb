{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Do not forget to change the runtime to A100**\n",
        "\n",
        "The Llama model is very large and has 3.8 Billion parameters. Also, as the size of the dataset increases we need RAM large enough to fit the models. To run the following training we need A100 processing units."
      ],
      "metadata": {
        "id": "_L8iSsD9XQ1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The following note book is re-used from** https://github.com/AI4Finance-Foundation/FinGPT"
      ],
      "metadata": {
        "id": "f37IqLMzB50L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install all required packages.\n",
        "peft package is needed to run Lora (Low-rank adaptation (LoRA) is a way to adapt a large machine learning model for specific uses without retraining the entire model.)\n"
      ],
      "metadata": {
        "id": "jXr3Ld5RVvav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install transformers --upgrade\n",
        "!pip install accelerate\n",
        "!pip install -U bitsandbytes\n",
        "!pip install loguru\n",
        "!pip install --upgrade peft\n",
        "# !pip install transformers==4.40.1"
      ],
      "metadata": {
        "id": "U6-5rNf_Pjty",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the version of transformers. Make sure it is >=4.40"
      ],
      "metadata": {
        "id": "mzQLHsgw-xK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install protobuf  cpm_kernels torch>=2.0 gradio mdtex2html sentencepiece accelerate"
      ],
      "metadata": {
        "id": "SqmiHTsOaN73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ],
      "metadata": {
        "id": "gmbzG-EueByq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Check CUDA availability and set device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Warning: CUDA is not available. Using CPU instead.\")"
      ],
      "metadata": {
        "id": "nbhy-5AOZH_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restart the session for transformers and bitsandbytes to take effect\n"
      ],
      "metadata": {
        "id": "uVqEBDxBV8mZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "create a huggingFace api_key and save it in the secrets as HF_TOKEN\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "l9vP7jMrVq0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "import shutil\n",
        "\n",
        "# Retrieve the token from Colab Secrets\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "os.environ[\"HF_TOKEN\"] = hf_token"
      ],
      "metadata": {
        "id": "p27wHFR9HdEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(token=hf_token)"
      ],
      "metadata": {
        "id": "fPlObo-RbqzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following creates a data folder.\n",
        "\n",
        "Load and Prepare Dataset:\n",
        "\n",
        "Import necessary libraries from the datasets package\n",
        "Load the Twitter Financial News Sentiment (TFNS) dataset and convert it to a Pandas dataframe.\n",
        "\n",
        "Map numerical labels to their corresponding sentiments. Here we have 3 categories: (negative, positive, neutral).\n",
        "\n",
        "Note: Though LLMs are capable of classifying into multiple categories, for higher accuracy, it is ideal to restrict number of classes.\n",
        "\n",
        "Add instruction for each data entry, which is crucial for Instruction Tuning.\n",
        "Convert the Pandas dataframe back to a Hugging Face Dataset object.\n",
        "\n",
        "The following code creates a data folder in the present working path\n",
        "\n",
        "Processed\n"
      ],
      "metadata": {
        "id": "iVlRdYT-MkYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "import shutil\n",
        "\n",
        "if not os.path.exists('./data'):\n",
        "    os.makedirs('./data')\n",
        "\n",
        "\n",
        "jsonl_path = \"../data/dataset_new.jsonl\"\n",
        "save_path = '../data/dataset_new'\n",
        "\n",
        "\n",
        "if os.path.exists(jsonl_path):\n",
        "    os.remove(jsonl_path)\n",
        "\n",
        "if os.path.exists(save_path):\n",
        "    shutil.rmtree(save_path)\n",
        "\n",
        "directory = \"../data\"\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)"
      ],
      "metadata": {
        "id": "a5EPAXpMHqWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l ./data/dataset_new"
      ],
      "metadata": {
        "id": "GovL_iL9cCKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "voFMZOd9cTCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Load and Prepare Dataset"
      ],
      "metadata": {
        "id": "4ZK2N8zycagq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "IDkqd-41INFL",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import datasets"
      ],
      "metadata": {
        "id": "pUKd4ukQt4m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic = {\n",
        "    0:'negative',\n",
        "    1:'positive',\n",
        "    2:'neutral'\n",
        "}\n",
        "\n",
        "tfns = load_dataset('zeroshot/twitter-financial-news-sentiment') #tfns = Twitter Financial News Sentiment\n",
        "\n",
        "tfns = tfns['train']\n",
        "tfns = tfns.to_pandas()\n",
        "\n",
        "tfns['label'] = tfns['label'].apply(lambda x : dic[x])  # Map numerical labels to their corresponding sentiments\n",
        "\n",
        "#Add prompt instruction for each data entry, which is crucial for Instruction Tuning.\n",
        "tfns['instruction'] = 'What is the sentiment of this tweet? Please choose an answer from {negative/neutral/positive}.'\n",
        "tfns.columns = ['input','output','instruction']\n",
        "\n",
        "#Convert the Pandas dataframe back to a Hugging Face Dataset object.\n",
        "tfns = datasets.Dataset.from_pandas(tfns)\n",
        "tfns"
      ],
      "metadata": {
        "id": "ezaVUeFbcni0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_dataset = datasets.concatenate_datasets([tfns]*2) #Create a list that contains 2 tfns\n",
        "train_dataset = tmp_dataset\n",
        "print(tmp_dataset.num_rows)"
      ],
      "metadata": {
        "id": "7qAt7eKac17g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_dataset = train_dataset.shuffle(seed = 42)\n",
        "all_dataset.shape"
      ],
      "metadata": {
        "id": "mslQ8g2Rc129"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Dataset Formatting and Tokenization"
      ],
      "metadata": {
        "id": "IhSmfgendFy0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Dataset Formatting"
      ],
      "metadata": {
        "id": "OYBYi_dAdHfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You must structure your data in a specific format that aligns with the training process."
      ],
      "metadata": {
        "id": "nk2jLLvRdLWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "9UT-eptVc1zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_examle(example:dict) -> dict:    #Defines a function named format_example that takes a dictionary as input (example: dict) and returns a dictionary (-> dict).\n",
        "  context = f\"Instruction:{example['instruction']}\\n\"   #Initializes a string variable context using an f-string to format the instruction.\n",
        "  if example.get('input'):     #Checks if the example dictionary has an input key and whether it contains a value.\n",
        "    context += f\"Input:{example['input']}\\n\"\n",
        "  context += 'Answer: '\n",
        "  target = example['output']\n",
        "  return {\"context\": context , \"target\":target}  # This is the format of json data.\n",
        "\n",
        "\n",
        "\n",
        "data_list = []\n",
        "for item in all_dataset.to_pandas().itertuples():    #Iterates over each row of the dataset all_dataset, which has been converted into a Pandas DataFrame using .to_pandas().\n",
        "  tmp = {}\n",
        "  tmp['instruction'] = item.instruction\n",
        "  tmp['input'] = item.input\n",
        "  tmp['output'] = item.output\n",
        "  data_list.append(tmp)"
      ],
      "metadata": {
        "id": "45YQpNgcc1ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_list[0])"
      ],
      "metadata": {
        "id": "Dwz2nQxjc1rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save to a json file\n",
        "from tqdm.notebook import tqdm\n",
        "import json\n",
        "\n",
        "with open(\"../data/dataset_new.jsonl\",'w') as f:\n",
        "  for example in tqdm(data_list,desc = 'formatting..'):\n",
        "    f.write(json.dumps(format_examle(example)) + '\\n')"
      ],
      "metadata": {
        "id": "HOu5bJsDc1kT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_data_list = []  # Var to save json data\n",
        "\n",
        "# Save to a jsonl file and store in json_data_list\n",
        "with open(\"../data/dataset_new.jsonl\", 'r') as f:\n",
        "    for line in f:\n",
        "        json_line = json.loads(line.strip())\n",
        "        json_data_list.append(json_line)"
      ],
      "metadata": {
        "id": "5dvjE0H8df_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(json_data_list[0]['target'], json_data_list[0]['context'])"
      ],
      "metadata": {
        "id": "DUuDyKbQdkg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Tokenization"
      ],
      "metadata": {
        "id": "jddoe3bDdyMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is the process of converting input text into tokens that can be fed into the model."
      ],
      "metadata": {
        "id": "wqIMIwfud0UW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6CzwKFaRduYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoConfig"
      ],
      "metadata": {
        "id": "_8GklgYPduVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'meta-llama/Meta-Llama-3-8B'   #Specifies the model you're working with\n",
        "jsonl_path = '../data/dataset_new.jsonl'\n",
        "save_path = '../data/dataset_new'    #The path where the processed dataset will be saved after tokenization or any other processing\n",
        "max_seq_length = 512    #Maximum sequence length for the inputs. If an input exceeds this length, it will either be truncated or skipped.\n",
        "skip_overlength = True    #A flag that determines whether to skip overlength examples that exceed max_seq_length"
      ],
      "metadata": {
        "id": "XZ6DeUm5duRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This preprocess function tokenizes the promt and target, combines them into Input ids, trims or pads the squence to the maximum squence length."
      ],
      "metadata": {
        "id": "KFNJLyUUePxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(tokenizer, config, example, max_seq_length):\n",
        "  prompt = example['context']\n",
        "  target = example['target']\n",
        "  prompt_ids = tokenizer.encode(   #ids refers to the numerical identifiers that correspond to tokens.These token ids are what the model processes, as models require numerical input rather than raw text.\n",
        "      prompt,\n",
        "      max_length = max_seq_length,\n",
        "      truncation = True\n",
        "      )\n",
        "  target_ids = tokenizer.encode(\n",
        "      target,\n",
        "      max_length = max_seq_length,\n",
        "      truncation = True,\n",
        "      add_special_tokens = False\n",
        "      )\n",
        "  input_ids = prompt_ids + target_ids + [config.eos_token_id]  #[config.eos_token_id] is a sign that marks the end of the list.\n",
        "  return {'input_ids':input_ids,'seq_len':len(prompt_ids)}"
      ],
      "metadata": {
        "id": "38yePEQTduOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "config = AutoConfig.from_pretrained(model_name, trust_remote_code=True, device_map='auto')"
      ],
      "metadata": {
        "id": "NUp52kmaduK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check\n",
        "example = json_data_list[0]\n",
        "prompt = example['context']\n",
        "target = example['target']\n",
        "\n",
        "example['target']"
      ],
      "metadata": {
        "id": "hRU3o-uCduHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "input_ids is a complete list of token IDs that combines the input sentence (prompt), the target sentence (target), and the end-of-sequence token (eos_token_id).\n",
        "This list is fed into the model for training or inference. The model uses these IDs to understand and process the input and generate the corresponding output."
      ],
      "metadata": {
        "id": "GRKXKWOoeh0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The read_jsonl function reads each line from the JSONL file, preprocesses it using the preprocess function,\n",
        "and then yields each preprocessed example."
      ],
      "metadata": {
        "id": "P2rXd-OtejSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_jsonl(path, max_seq_length, skip_overlength=False):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(    #Initializes a tokenizer using a pre-trained model specified by model_name.\n",
        "        model_name, trust_remote_code=True)\n",
        "    config = AutoConfig.from_pretrained(    #Loads the configuration for the model. device_map='auto' helps automatically map the model to available devices (e.g., GPU or CPU).\n",
        "        model_name, trust_remote_code=True, device_map='auto')\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in tqdm(f.readlines()):\n",
        "            example = json.loads(line)\n",
        "            #Preprocesses each example by tokenizing it and converting it into input_ids using the preprocess() function,\n",
        "            #which takes the tokenizer, config, example, and max_seq_length as inputs.\n",
        "            feature = preprocess(tokenizer, config, example, max_seq_length)\n",
        "            if skip_overlength and len(feature[\"input_ids\"]) > max_seq_length:\n",
        "                continue\n",
        "            feature[\"input_ids\"] = feature[\"input_ids\"][:max_seq_length]  #Truncates the input_ids to ensure they do not exceed max_seq_length.\n",
        "            yield feature\n",
        "#Uses yield to return one preprocessed feature at a time, making the function a generator.\n",
        "#This allows you to iterate over the processed features one by one without loading everything into memory at once."
      ],
      "metadata": {
        "id": "wCuXwS4yduEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Save the Dataset"
      ],
      "metadata": {
        "id": "f7OrGDpSe_9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gy9AbYU0fCTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = './data/dataset_new'"
      ],
      "metadata": {
        "id": "zWNP6gBgfCDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.Dataset.from_generator(\n",
        "    lambda: read_jsonl(jsonl_path, max_seq_length, skip_overlength)\n",
        "    )\n",
        "dataset.save_to_disk(save_path)"
      ],
      "metadata": {
        "id": "_lI0-c0OfB_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk\n",
        "\n",
        "# Load Dataset\n",
        "loaded_dataset = load_from_disk('./data/dataset_new')\n",
        "\n",
        "# Check the structure of Dataset\n",
        "print(loaded_dataset)\n",
        "\n",
        "# Print the first sample of the dataset\n",
        "print(loaded_dataset['input_ids'][0])"
      ],
      "metadata": {
        "id": "byToGk2ofB8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Save dataset to your own google drive\n",
        "\n",
        "Every time you restart colab, you don't have to reformat the data, you can just load the formatted data directly from this google drive."
      ],
      "metadata": {
        "id": "ODUkZTtsfReT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') #You'll be asked to authorize access to your Google Drive"
      ],
      "metadata": {
        "id": "Z9NXvurIfB5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = '/content/drive/MyDrive/Colab Notebooks/AI4Finance/FinGPT/FinGPT: Training with LoRA and Llama-3/dataset_new' #Change to your own address\n",
        "# Write your own Google drive saving address in xxxxxxxx part: '/content/drive/MyDrive/xxxxxxxxxxxxxxxxx/dataset_new'\n",
        "dataset.save_to_disk(save_path)"
      ],
      "metadata": {
        "id": "lSmhYP5efB12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pGjbZUOhfBy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Setup FinGPT training with LoRA and Llama-3\n",
        "\n",
        "### 3.1 Training Arguments Setup:\n",
        "Initialize and set training arguments."
      ],
      "metadata": {
        "id": "KS28yN9-f-xJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Optional\n",
        "import torch\n",
        "from loguru import logger\n",
        "from transformers import (\n",
        "    AutoModel,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoModelForCausalLM\n",
        ")\n",
        "from peft import (\n",
        "    TaskType,\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    set_peft_model_state_dict,\n",
        "    prepare_model_for_kbit_training,\n",
        ")\n",
        "from peft.utils import TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING\n",
        "from transformers import LlamaForCausalLM"
      ],
      "metadata": {
        "id": "FLbnfIzQfBvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/Colab Notebooks/AI4Finance/FinGPT/FinGPT: Training with LoRA and Llama-3/finetuned_model/',    # Path to save the fine-tuned model\n",
        "    logging_steps = 500,               # Log every 500 steps\n",
        "    # max_steps=10000,                 # Maximum number of training steps (commented out, can be enabled)\n",
        "    num_train_epochs = 2,              # Number of training epochs (train for 2 epochs)\n",
        "    per_device_train_batch_size=4,     # Batch size of 4 for training on each device (GPU/CPU)\n",
        "    gradient_accumulation_steps=8,     # Accumulate gradients for 8 steps before updating weights\n",
        "    learning_rate=1e-4,                # Learning rate set to 1e-4\n",
        "    weight_decay=0.01,                 # Weight decay (L2 regularization) set to 0.01\n",
        "    warmup_steps=1000,                 # Warm up the learning rate for the first 1000 steps\n",
        "    # save_steps=500,\n",
        "    fp16=True,                         # Enable FP16 mixed precision training to save memory and speed up training\n",
        "    # bf16=True,                       # Enable BF16 mixed precision training (commented out)\n",
        "    torch_compile = False,             # Whether to enable Torch compile (`False` means not enabled)\n",
        "    load_best_model_at_end = True,     # Load the best-performing model at the end of training\n",
        "    # evaluation_strategy=\"steps\",       # Evaluation strategy is set to evaluate every few steps\n",
        "    eval_strategy=\"steps\",\n",
        "    save_steps=500,  # # Save the model every 500 steps\n",
        "    metric_for_best_model=\"loss\",\n",
        "    remove_unused_columns=False,       # Whether to remove unused columns during training (keep all columns)\n",
        "    logging_dir=\"./logs\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "3R5vh8f7fBr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Quantization Config Setup:\n",
        "Set quantization configuration to reduce model size without losing significant precision."
      ],
      "metadata": {
        "id": "UN4_fNEwgRA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list | grep bitsandbytes"
      ],
      "metadata": {
        "id": "LbVggu-UfBoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quantitative allocation\n",
        "q_config = BitsAndBytesConfig(load_in_4bit=False,\n",
        "                                bnb_4bit_quant_type='nf4',\n",
        "                                bnb_4bit_use_double_quant=True,\n",
        "                                bnb_4bit_compute_dtype=torch.float16\n",
        "                                )"
      ],
      "metadata": {
        "id": "VRcvuiFMduAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
      ],
      "metadata": {
        "id": "-EHBN-kUdt8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Model Loading & Preparation:\n",
        "Load the base model and tokenizer, and prepare the model for INT8 training.\n",
        "\n",
        "Runtime -> Change runtime type -> A100 GPU\n",
        "\n",
        "Restart runtime and run again if not working"
      ],
      "metadata": {
        "id": "qdUwLBj1gieu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "D03m4vQXghTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.utils import is_bitsandbytes_available\n",
        "is_bitsandbytes_available()"
      ],
      "metadata": {
        "id": "hrdO_ObyghPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LlamaForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config = q_config,\n",
        "        trust_remote_code=True,\n",
        "        device_map='auto'\n",
        "    )"
      ],
      "metadata": {
        "id": "rPgumXfFghKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# # Check CUDA availability and set device\n",
        "# if torch.cuda.is_available():\n",
        "#     device = torch.device(\"cuda\")\n",
        "# else:\n",
        "#     device = torch.device(\"cpu\")\n",
        "#     print(\"Warning: CUDA is not available. Using CPU instead.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "iUlrheG9YjKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 LoRA Config & Setup"
      ],
      "metadata": {
        "id": "BVkVv4qxhqAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "# LoRA for Llama3\n",
        "target_modules = TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING['llama']  # Modules for the Llama model\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=target_modules,\n",
        "    bias='none',\n",
        ")\n",
        "\n",
        "# Loading LoRA for Llama3 models using PEFT (Parameter-Efficient Fine-Tuning)\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print the number of trainable parameters\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "RJQ8GjSJghG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume_from_checkpoint = None\n",
        "if resume_from_checkpoint is not None:\n",
        "    checkpoint_name = os.path.join(resume_from_checkpoint, 'pytorch_model.bin')\n",
        "    if not os.path.exists(checkpoint_name):\n",
        "        checkpoint_name = os.path.join(\n",
        "            resume_from_checkpoint, 'adapter_model.bin'\n",
        "        )\n",
        "        resume_from_checkpoint = False\n",
        "    if os.path.exists(checkpoint_name):\n",
        "        logger.info(f'Restarting from {checkpoint_name}')\n",
        "        adapters_weights = torch.load(checkpoint_name)\n",
        "        set_peft_model_state_dict(model, adapters_weights)\n",
        "    else:\n",
        "        logger.info(f'Checkpoint {checkpoint_name} not found')"
      ],
      "metadata": {
        "id": "KMD_AxfZghDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "rZjCcBdoghAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Loading Data and Training FinGPT\n",
        "\n",
        "\n",
        "In this segment, we'll delve into the loading of your pre-processed data, and finally, launch the training of your FinGPT model. Here's a stepwise breakdown of the script provided:\n",
        "\n",
        "\n",
        "\n",
        "*   Need to purchase Google Colab GPU plans, Colab Pro is\n",
        "sufficient or just buy 100 compute units for $10"
      ],
      "metadata": {
        "id": "RhQQHQYLiDCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Loading Your Data:"
      ],
      "metadata": {
        "id": "iEIehSTniHf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "from datasets import load_from_disk\n",
        "import datasets\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive') # You will be asked to authorize access to your Google Drive\n",
        "\n",
        "save_path = '/content/drive/MyDrive/Collaboration_Jig_San/Chapter_2/AI4Finance/FinGPT/FinGPT: Training with LoRA and Llama-3/dataset_new'\n",
        "# Load saved dataset\n",
        "dataset = load_from_disk(save_path)\n",
        "dataset = dataset.train_test_split(0.2, shuffle=True, seed = 42)"
      ],
      "metadata": {
        "id": "4lOk3QUzgg9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Training Configuration and Launch:\n",
        "\n",
        "\n",
        "\n",
        "*   Customize the Trainer class for specific loss computation, prediction step, and model-saving methods.\n",
        "*   Define a data collator function to process batches of data during training.\n",
        "*   Set up TensorBoard for logging, instantiate your modified trainer, and begin training."
      ],
      "metadata": {
        "id": "nILTd8fKiQTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "SlquB0eugg57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_collator(features: list) -> dict:\n",
        "    # Check if pad_token_id is None, if it is then use eos_token_id as the padding value\n",
        "    if tokenizer.pad_token_id is None:\n",
        "        pad_token_id = tokenizer.eos_token_id  # Use eos_token_id as a fill symbol\n",
        "    else:\n",
        "        pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    len_ids = [len(feature[\"input_ids\"]) for feature in features]\n",
        "    longest = max(len_ids)\n",
        "\n",
        "    input_ids = []\n",
        "    labels_list = []\n",
        "\n",
        "    for ids_l, feature in sorted(zip(len_ids, features), key=lambda x: -x[0]):\n",
        "        ids = feature[\"input_ids\"]\n",
        "        seq_len = feature[\"seq_len\"]\n",
        "\n",
        "        # Padding with calculated pad_token_id\n",
        "        labels = (\n",
        "            [pad_token_id] * (seq_len - 1) + ids[(seq_len - 1) :] + [pad_token_id] * (longest - ids_l)\n",
        "        )\n",
        "        ids = ids + [pad_token_id] * (longest - ids_l)\n",
        "\n",
        "        _ids = torch.LongTensor(ids)\n",
        "        labels_list.append(torch.LongTensor(labels))\n",
        "        input_ids.append(_ids)\n",
        "\n",
        "    input_ids = torch.stack(input_ids)\n",
        "    labels = torch.stack(labels_list)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"labels\": labels,\n",
        "    }"
      ],
      "metadata": {
        "id": "6Q_RLWXhgg2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from transformers.integrations import TensorBoardCallback"
      ],
      "metadata": {
        "id": "uaVO2cwYggzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "# Took about 10 compute units\n",
        "writer = SummaryWriter()\n",
        "# trainer = ModifiedTrainer(\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,             # Trainer args\n",
        "    train_dataset=dataset[\"train\"], # Training set\n",
        "    eval_dataset=dataset[\"test\"],   # Testing set\n",
        "    data_collator=data_collator,    # Data Collator\n",
        "    callbacks=[TensorBoardCallback(writer)],\n",
        ")\n",
        "trainer.train()\n",
        "writer.close()\n",
        "\n",
        "# Save model to Google Drive\n",
        "model_output_dir = '/content/drive/MyDrive/Collaboration_Jig_San/Chapter_2/AI4Finance/FinGPT/FinGPT: Training with LoRA and Llama-3/Model/'\n",
        "model.save_pretrained(model_output_dir)\n",
        "\n",
        "\n",
        "# model.save_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "S3dsQtDHggwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_output_dir = '/content/drive/MyDrive/Collaboration_Jig_San/Chapter_2/AI4Finance/FinGPT/FinGPT: Training with LoRA and Llama-3/Model/'\n",
        "model.save_pretrained(model_output_dir)"
      ],
      "metadata": {
        "id": "PVFEBYS0st3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now your model is trained and saved! You can download it and use it for generating financial insights or any other relevant tasks in the finance domain. The usage of TensorBoard allows you to deeply understand and visualize the training dynamics and performance of your model in real-time."
      ],
      "metadata": {
        "id": "i9WkCp5wj_7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Inference and Benchmarks using FinGPT\n",
        "\n",
        "Now that your model is trained, letâ€™s understand how to use it to infer and run benchmarks.\n",
        "\n",
        "\n",
        "*   Took about 10 compute units"
      ],
      "metadata": {
        "id": "jEwKIjnqkKLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Load the model"
      ],
      "metadata": {
        "id": "GDHIoYQFkS2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#clone the FinNLP repository\n",
        "!git clone https://github.com/AI4Finance-Foundation/FinNLP.git\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/FinNLP/')"
      ],
      "metadata": {
        "id": "i_6erAzwkB8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# Load benchmark datasets from FinNLP\n",
        "from finnlp.benchmarks.fpb import test_fpb\n",
        "from finnlp.benchmarks.fiqa import test_fiqa , add_instructions\n",
        "from finnlp.benchmarks.tfns import test_tfns\n",
        "from finnlp.benchmarks.nwgi import test_nwgi"
      ],
      "metadata": {
        "id": "Vw6WE_91kB4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model from google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8ORU3dwDkB1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Fine-tuned PEFT model paths\n",
        "path_to_check = '/content/drive/MyDrive/Collaboration_Jig_San/Chapter_2/AI4Finance/FinGPT/FinGPT: Training with LoRA and Llama-3/Model/'\n",
        "\n",
        "# Check if the specified path exists\n",
        "if os.path.exists(path_to_check):\n",
        "    print(\"Path exists.\")\n",
        "else:\n",
        "    print(\"Path does not exist.\")"
      ],
      "metadata": {
        "id": "2Ay3N-3skByK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "_CMtj_SPkBuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# login into hf\n",
        "login(token=hf_token)"
      ],
      "metadata": {
        "id": "mG4WCaTBkBrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def eval_with_PEFT_model(base_model,peft_model):\n",
        "\n",
        "\n",
        "  # Loda tokenizer\n",
        "  tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
        "  tokenizer.pad_token = tokenizer.eos_token  # Use eos_token as pad_token\n",
        "  tokenizer.padding_side = 'left'  # Important: Set as left padding\n",
        "\n",
        "\n",
        "  model = LlamaForCausalLM.from_pretrained(base_model,\n",
        "                                          trust_remote_code=True,\n",
        "                                          load_in_8bit=True,\n",
        "                                          device_map=\"cuda:0\")  #Set the model to GPU\n",
        "\n",
        "  # load peft's fine-tuned model weights\n",
        "  model = PeftModel.from_pretrained(model, peft_model)\n",
        "\n",
        "  return model.eval(), tokenizer"
      ],
      "metadata": {
        "id": "77Zo79adkBn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(path_to_check)"
      ],
      "metadata": {
        "id": "xzX8RFHjlKHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = \"meta-llama/Meta-Llama-3-8B\" # Loading the Llama base model and supporting text-generated models\n",
        "peft_model = path_to_check  # Fine-tuned PEFT model paths\n",
        "\n",
        "model,tokenizer = eval_with_PEFT_model(base_model,peft_model)"
      ],
      "metadata": {
        "id": "8wfj0Uk3ggtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Run Benchmarks:"
      ],
      "metadata": {
        "id": "xrfeQsl9lO6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "import logging\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)"
      ],
      "metadata": {
        "id": "7_4cbLbfggqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TFNS Test Set, len 2388\n",
        "# Available: 99.4 compute units\n",
        "res_tfns = test_tfns(model, tokenizer, batch_size = batch_size)\n",
        "# Available: 98.9 compute units\n",
        "# Took about 0.5 compute unite to inference"
      ],
      "metadata": {
        "id": "InR0qo5hggms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FPB, len 1212\n",
        "res_fpb = test_fpb(model, tokenizer, batch_size = batch_size)\n",
        "# since we are running on our dataset, select 'y'"
      ],
      "metadata": {
        "id": "FnTSfWc1k8K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FiQA, len 275\n",
        "res_fiqa = test_fiqa(model, tokenizer, prompt_fun = add_instructions, batch_size = batch_size)"
      ],
      "metadata": {
        "id": "wst5PCKVk8G5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NWGI, len 4047\n",
        "res_nwgi = test_nwgi(model, tokenizer, batch_size = batch_size)"
      ],
      "metadata": {
        "id": "iMhUz6xwk8C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_nwgi"
      ],
      "metadata": {
        "id": "XvmNP1lqk7_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score,f1_score\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "7WUYumNSk77c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_score(df):\n",
        "\n",
        "  accuracy = accuracy_score(df['target'], df['new_out'])\n",
        "\n",
        "  f1_macro = f1_score(df['target'], df['new_out'], average='macro')\n",
        "\n",
        "  f1_weighted = f1_score(df['target'], df['new_out'], average='weighted')\n",
        "\n",
        "  return round(accuracy,3), round(f1_macro,3), round(f1_weighted,3)\n",
        "\n",
        "\n",
        "def form_socre_dic(dataset_name):\n",
        "  score_list = get_score(dataset_name)\n",
        "\n",
        "  score_dic = {\n",
        "        'Accuracy': score_list[0],\n",
        "        'F1_macro': score_list[1],\n",
        "        'F1_weighted': score_list[2]\n",
        "  }\n",
        "\n",
        "  return score_dic"
      ],
      "metadata": {
        "id": "rOqdA-Xuk738"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  score_dic = {\n",
        "        'TFNS': form_socre_dic(res_tfns),\n",
        "        'FPB': form_socre_dic(res_fpb),\n",
        "        'FIQA': form_socre_dic(res_fiqa),\n",
        "        'NWGI': form_socre_dic(res_nwgi)\n",
        "  }"
      ],
      "metadata": {
        "id": "OW0mSt2nk70M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(score_dic)"
      ],
      "metadata": {
        "id": "BKAXU-Y8k7wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_dic"
      ],
      "metadata": {
        "id": "Kh3npjSKk7ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Result comparision"
      ],
      "metadata": {
        "id": "67cljVC7lyhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Results of other fine-tuned model come from previous tranning results.\n",
        "results = {\n",
        "    \"TFNS\": {\n",
        "        \"FinGPT-ChatGlm2-6b\": {\"Acc\": 0.856, \"F1_macro\": 0.806, \"F1_weighted\": 0.850},\n",
        "        \"FinGPT-V3.1\": {\"Acc\": 0.876, \"F1_macro\": 0.841, \"F1_weighted\":  0.875},\n",
        "    },\n",
        "    \"FPB\": {\n",
        "        \"FinGPT-ChatGlm2-6b\": {\"Acc\": 0.741, \"F1_macro\": 0.655, \"F1_weighted\": 0.694},\n",
        "        \"FinGPT-V3.1\": {\"Acc\": 0.856, \"F1_macro\": 0.841, \"F1_weighted\": 0.855},\n",
        "    },\n",
        "    \"FIQA\": {\n",
        "        \"FinGPT-ChatGlm2-6b\": {\"Acc\": 0.48, \"F1_macro\": 0.5,  \"F1_weighted\": 0.49},\n",
        "        \"FinGPT-V3.1\": {\"Acc\": 0.836, \"F1_macro\":0.746, \"F1_weighted\": 0.850},\n",
        "    },\n",
        "    \"NWGI\": {\n",
        "        \"FinGPT-ChatGlm2-6b\": {\"Acc\": 0.521, \"F1_macro\": 0.500, \"F1_weighted\":0.490},\n",
        "        \"FinGPT-V3.1\": {\"Acc\": 0.642, \"F1_macro\": 0.650,\"F1_weighted\": 0.642},\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "klO6SD8Mk7pL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the results dictionary to insert the value of FinGPT-Llama-8b.\n",
        "for dataset_name, scores in score_dic.items():\n",
        "    if dataset_name in results:\n",
        "\n",
        "        if \"FinGPT-Llama-8b\" not in results[dataset_name]:\n",
        "            results[dataset_name][\"FinGPT-Llama-8b\"] = {}\n",
        "\n",
        "        results[dataset_name][\"FinGPT-Llama-8b\"].update({\n",
        "            \"Acc\": scores['Accuracy'],\n",
        "            \"F1_macro\": scores['F1_macro'],\n",
        "            \"F1_weighted\": scores['F1_weighted']\n",
        "        })"
      ],
      "metadata": {
        "id": "HOcqaHxmk7lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for dataset, models in results.items():\n",
        "    for model, metrics in models.items():\n",
        "        data.append([dataset, model, metrics.get(\"Acc\", None), metrics.get(\"F1_macro\", None),\n",
        "                     metrics.get(\"F1_micro\", None), metrics.get(\"F1_weighted\", None)])\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"Dataset\", \"Model\", \"Acc\", \"F1_macro\", \"F1_micro\", \"F1_weighted\"])\n",
        "\n",
        "# visualization\n",
        "def plot_metric(metric_name):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for model in df[\"Model\"].unique():\n",
        "        subset = df[df[\"Model\"] == model]\n",
        "        plt.plot(subset[\"Dataset\"], subset[metric_name], marker='o', label=model)\n",
        "    plt.title(f\"{metric_name} Comparison Across Datasets\")\n",
        "    plt.xlabel(\"Dataset\")\n",
        "    plt.ylabel(metric_name)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Visualization of Accuracy, F1_macro and F1_weighted comparison\n",
        "plot_metric(\"Acc\")\n",
        "plot_metric(\"F1_macro\")\n",
        "plot_metric(\"F1_weighted\")"
      ],
      "metadata": {
        "id": "q1j-kI0ok7h2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transpose the data table so that the rows are datasets and the columns are models for Acc, F1_macro, and F1_weighted, respectively.\n",
        "\n",
        "\n",
        "acc_df = df.pivot(index='Dataset', columns='Model', values='Acc')\n",
        "\n",
        "\n",
        "f1_macro_df = df.pivot(index='Dataset', columns='Model', values='F1_macro')\n",
        "\n",
        "\n",
        "f1_weighted_df = df.pivot(index='Dataset', columns='Model', values='F1_weighted')"
      ],
      "metadata": {
        "id": "h88enUDfk7dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy DataFrame:\")\n",
        "acc_df"
      ],
      "metadata": {
        "id": "Bk4-gXHWggi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nF1 Macro DataFrame:\")\n",
        "f1_macro_df"
      ],
      "metadata": {
        "id": "fz4D-ozLl5Kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nF1 Weighted DataFrame:\")\n",
        "f1_weighted_df"
      ],
      "metadata": {
        "id": "ub6yA9FTl5EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# upto here we have the code from FinGPT training LoRA"
      ],
      "metadata": {
        "id": "jYOMN_Ks5Juy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dnkbkYI6l5BN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l2W-3ZV2l49m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UGAzLpnFl46O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yCU-SorOl422"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WJcmbw_Fl4zZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FdBIovdBl4ve"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}